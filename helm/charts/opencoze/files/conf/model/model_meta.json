{
    "provider2models": {
        "Claude": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    }
                ]
            }        
        },
        "GPT": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 400000
                },
                "capability": {
                    "cot_display": false,
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false,
                    "prefill_resp": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    }
                ]
            }
        },
        "Gemini": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 204800
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": true,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.94",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            }
        },
        "QWen": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1.99",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0.01",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.8",
                            "creative": "0.8",
                            "balance": "0.8",
                            "precise": "0.8"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "输出格式",
                        "desc": "- **文本**: 使用普通文本格式回复\n- **Markdown**: 将引导模型使用Markdown格式输出回复\n- **JSON**: 将引导模型使用JSON格式输出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            }
        },
        "SEED": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 229376
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答长度",
                        "desc": "控制模型思维链推理和回复输出的最大长度（单位 token）。配置了该参数后，可以让模型输出超长内容，max_tokens （最大回复长度，默认值 4k）与思维链最大长度将失效，模型按需输出内容，直到达到“最大推理&回复长度”（max_completion_tokens） 配置的值。\n注意：若与“最大回复长度”（max_tokens） 字段同时设置，则“最大回复长度”不会生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            }
        },
        "Llama": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            }
        },
        "DeekSeek": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "deepseek-reasoner": {
                "display_info": {
                    "name": "DeepSeek-R1·工具调用",
                    "description": {
                        "zh_cn": "R1 functionCall 版本，支持在Single-Agent模式下调用各类扣子工具（插件、工作流、知识库等）。",
                        "en_us": "DeepSeek-R1 functionCall version, which supports calling various Coze tools (plugins, workflows, knowledge bases, etc.) in Single-Agent mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2200"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "deepseek-chat": {
                "display_info": {
                    "name": "DeepSeek-V3·工具调用",
                    "description": {
                        "zh_cn": "V3 functionCall 版本，支持在Single-Agent模式下调用各类扣子工具（插件、工作流、知识库等）。",
                        "en_us": "DeepSeek-V3 functionCall version, which supports calling various Coze tools (plugins, workflows, knowledge bases, etc.) in Single-Agent mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 65536
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            }
        }
    }
}